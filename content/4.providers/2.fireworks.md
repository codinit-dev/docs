---
title: Fireworks AI
description: Configure Fireworks AI with CodinIT to access ultra-fast inference of leading open-source AI models including Llama 3.1, Mixtral, Code Llama, and more.
navigation:
  icon: i-lucide-zap
---

Configure Fireworks AI with CodinIT to access ultra-fast inference of leading open-source AI models including Llama 3.1, Mixtral, Code Llama, and more. This guide covers account setup, API key generation, and integration for teams prioritizing speed and production performance.

## Overview

::callout{color="primary"}
**Fireworks AI** is a high-performance inference platform engineered for production workloads, providing blazing-fast access to optimized open-source AI models—including Llama 3.1, Mixtral 8x7B, Code Llama, and Phi-3—with industry-leading response times and reliability.

[Learn more about Fireworks AI](https://fireworks.ai/)
::

This guide is designed for development teams who prioritize speed, reliability, and production-ready AI inference with optimized model hosting and enterprise-grade performance.

## Step 1: Create Your Fireworks AI Account and Generate API Keys

### 1.1 Sign Up for Fireworks AI

::steps{level="4"}

#### Visit Fireworks AI

Go to [Fireworks AI Platform](https://fireworks.ai/)

#### Create Your Account

- Click "Get Started" or "Sign Up"
- Register with your email or GitHub account
- Complete email verification and profile setup
- Accept terms of service and usage policies

::

### 1.2 Navigate to API Key Generation

::steps{level="4"}

#### Access Your Dashboard

- Log into your Fireworks AI account
- Navigate to the main dashboard
- Click on "API Keys" in the left sidebar or settings menu

#### Generate New API Key

- Click "Create API Key" or "New API Key"
- Provide a descriptive name (e.g., "CodinIT Development", "Production App")
- Set permissions and usage scopes if available
- Copy and securely store your generated API key

::

### 1.3 API Key Security and Best Practices

::card-group
  :::card
  ---
  icon: i-lucide-shield
  title: Secure Storage
  ---
  Store API keys in environment variables or secure credential managers. Never hard-code API keys in source code.
  :::

  :::card
  ---
  icon: i-lucide-key
  title: Access Control
  ---
  Monitor API key usage through the dashboard. Implement key rotation policies and set up usage alerts.
  :::

  :::card
  ---
  icon: i-lucide-users
  title: Team Management
  ---
  Create separate API keys for different team members or projects with descriptive naming conventions.
  :::
::

## Step 2: Explore High-Performance Models and Capabilities

### 2.1 Optimized Model Catalog

Fireworks AI specializes in ultra-fast inference of carefully optimized open-source models:

::tabs
  :::tabs-item{label="Large Language Models" icon="i-lucide-brain"}
  - **Llama 3.1 405B** - Meta's flagship model with massive capability
  - **Llama 3.1 70B** - High-performance balanced model
  - **Llama 3.1 8B** - Lightning-fast responses for most use cases
  :::

  :::tabs-item{label="Mixture of Experts" icon="i-lucide-network"}
  - **Mixtral 8x7B** - Efficient sparse model with excellent performance
  - **Mixtral 8x22B** - Advanced MoE with enhanced capabilities
  - **Phi-3 Medium** - Microsoft's efficient reasoning model
  :::

  :::tabs-item{label="Code-Specialized" icon="i-lucide-code"}
  - **Code Llama 34B** - Advanced code generation and completion
  - **Code Llama 7B** - Fast coding assistance and debugging
  - **StarCoder 15B** - Specialized programming model
  :::

  :::tabs-item{label="Conversation & Instruct" icon="i-lucide-message-circle"}
  - **Llama 3.1 70B Instruct** - Optimized for chat and dialogue
  - **Mixtral 8x7B Instruct** - Fast, multilingual conversation
  - **Yi 34B Chat** - Advanced reasoning in conversations
  :::
::

### 2.2 Performance and Speed Advantages

::card-group
  :::card
  ---
  icon: i-lucide-zap
  title: Ultra-Fast Inference
  ---
  Industry-leading response times with optimized model hosting
  :::

  :::card
  ---
  icon: i-lucide-shield-check
  title: Production-Ready
  ---
  Built for high-throughput applications with reliable uptime
  :::

  :::card
  ---
  icon: i-lucide-cpu
  title: Optimized Infrastructure
  ---
  Custom GPU clusters designed for AI inference
  :::

  :::card
  ---
  icon: i-lucide-trending-up
  title: Scalable Performance
  ---
  Auto-scaling to handle traffic spikes and varying loads
  :::
::

### 2.3 Model Selection Strategy

::field-group
  :::field{name="Speed-Critical Applications" type="strategy"}
  Use 7B-8B models for sub-second responses
  :::

  :::field{name="Balanced Performance" type="strategy"}
  70B models for complex tasks with reasonable speed
  :::

  :::field{name="Maximum Capability" type="strategy"}
  405B models for the most demanding applications
  :::

  :::field{name="Code Generation" type="strategy"}
  Specialized Code Llama models for development workflows
  :::
::

## Step 3: Configure the CodinIT VS Code Extension

### 3.1 Install and Open CodinIT

::steps{level="4"}

#### Download VS Code

Go to [Download Visual Studio Code](https://code.visualstudio.com/)

#### Install the CodinIT Extension

- Open VS Code
- Navigate to the Extensions Marketplace (Ctrl+Shift+X or Cmd+Shift+X)
- Search for **CodinIT** and install the extension

::

### 3.2 Configure CodinIT Settings

::steps{level="4"}

#### Open CodinIT Settings

Click the settings ⚙️ icon within the CodinIT extension

#### Set API Provider

Choose **Fireworks AI** from the API Provider dropdown

#### Enter Your API Key

Paste the API key you generated in Step 1

#### Select Your Model

Choose from available models (e.g., **Llama 3.1 70B Instruct** for balanced performance)

#### Configure Performance Settings

Adjust temperature, max tokens, and other parameters as needed

#### Save and Test

Save your settings and test with a prompt (e.g., "Create a Python function to sort a dictionary by values.")

::

## Step 4: Authentication Setup and Configuration

### Option A: Environment Variable (Recommended)

::tabs
  :::tabs-item{label="Windows (Command Prompt)" icon="i-lucide-terminal"}
  ```cmd
  set FIREWORKS_API_KEY=your_api_key_here
  ```
  :::

  :::tabs-item{label="Windows (PowerShell)" icon="i-lucide-terminal"}
  ```powershell
  $env:FIREWORKS_API_KEY="your_api_key_here"
  ```
  :::

  :::tabs-item{label="macOS/Linux" icon="i-lucide-terminal"}
  ```bash
  export FIREWORKS_API_KEY=your_api_key_here
  ```
  :::
::

::collapsible{title="Make Environment Variable Persistent"}

**On Windows:**
```cmd
setx FIREWORKS_API_KEY "your_api_key_here"
```

**On macOS/Linux (add to ~/.bashrc, ~/.zshrc, or ~/.bash_profile):**
```bash
echo 'export FIREWORKS_API_KEY="your_api_key_here"' >> ~/.bashrc
source ~/.bashrc
```

**Restart VS Code** to ensure it picks up the new environment variable

::

### Option B: Direct Configuration in CodinIT

::steps{level="4"}

#### Extension Settings

Open the CodinIT extension settings panel in VS Code

#### API Key Input

Enter your Fireworks AI API key directly in the API key field

#### Secure Storage

VS Code stores the API key securely in its encrypted settings storage

::

### Option C: Project-Based Configuration

::code-group
  :::code-block{label=".env" language="bash"}
  ```bash
  FIREWORKS_API_KEY=your_api_key_here
  ```
  :::

  :::code-block{label=".gitignore" language="bash"}
  ```bash
  # Environment variables
  .env
  .env.local
  .env.production
  .env.development
  ```
  :::

  :::code-block{label=".env.example" language="bash"}
  ```bash
  FIREWORKS_API_KEY=your_fireworks_api_key_here
  ```
  :::
::

## Step 5: Performance Optimization and Speed Maximization

### 5.1 Understanding Response Times and Throughput

::card-group
  :::card
  ---
  icon: i-lucide-clock
  title: Latency Metrics
  ---
  - 7B-8B models: 100-300ms average response time
  - 70B models: 500-1000ms average response time
  - Monitor performance in your dashboard
  :::

  :::card
  ---
  icon: i-lucide-activity
  title: Throughput Optimization
  ---
  - Implement request batching
  - Use streaming responses
  - Configure appropriate timeouts
  :::
::

### 5.2 Model-Specific Performance Tuning

::field-group
  :::field{name="Temperature" type="parameter"}
  Lower values (0.1-0.3) for consistent outputs, higher (0.7-1.0) for creativity
  :::

  :::field{name="Max Tokens" type="parameter"}
  Set appropriate limits to control response length and cost
  :::

  :::field{name="Top-p and Top-k" type="parameter"}
  Fine-tune for quality vs. speed trade-offs
  :::
::

**Prompt Engineering for Speed:**
- Write clear, concise prompts to reduce processing time
- Use system prompts effectively to provide context without repetition
- Implement prompt templates for consistent performance

### 5.3 Caching and Request Optimization

::card-group
  :::card
  ---
  icon: i-lucide-database
  title: Response Caching
  ---
  Implement local caching for repeated queries and semantic caching for similar prompts
  :::

  :::card
  ---
  icon: i-lucide-shuffle
  title: Request Patterns
  ---
  Batch similar requests, implement exponential backoff, and use async/await patterns
  :::
::

## Step 6: Cost Management and Usage Monitoring

### 6.1 Understanding Fireworks AI Pricing

::card-group
  :::card
  ---
  icon: i-lucide-dollar-sign
  title: Token-Based Pricing
  ---
  Pay per input and output token with transparent pricing. Different models have varying costs.
  :::

  :::card
  ---
  icon: i-lucide-trending-down
  title: Cost Optimization
  ---
  Use smaller models for simple tasks and implement prompt caching to reduce costs.
  :::
::

### 6.2 Usage Monitoring and Analytics

::field-group
  :::field{name="Dashboard Monitoring" type="feature"}
  Track API calls, token usage, costs, response times, and error rates in real-time
  :::

  :::field{name="Budget Management" type="feature"}
  Set monthly spending limits, track cost per component, and implement usage quotas
  :::
::

### 6.3 Rate Limits and Scaling

::callout{color="warning"}
**Understanding Limits**: Monitor current rate limits in your Fireworks AI dashboard. Understand both requests per minute and tokens per minute limits.
::

**Scaling Strategies:**
- Implement queue systems for high-volume applications
- Use load balancing across multiple API keys if needed
- Consider dedicated endpoints for enterprise workloads

## Step 7: Production Deployment and Enterprise Features

### 7.1 Production Readiness

::card-group
  :::card
  ---
  icon: i-lucide-shield
  title: Reliability Features
  ---
  Built-in redundancy, failover capabilities, and industry-leading uptime SLAs
  :::

  :::card
  ---
  icon: i-lucide-lock
  title: Security & Compliance
  ---
  Enterprise-grade security with SOC 2, GDPR compliance, and HTTPS encryption
  :::
::

### 7.2 Advanced Integration Patterns

::field-group
  :::field{name="Error Handling" type="pattern"}
  Implement comprehensive error handling, circuit breaker patterns, and monitoring
  :::

  :::field{name="Performance Monitoring" type="pattern"}
  Integrate with APM tools, track user-facing metrics, and implement A/B testing
  :::
::

### 7.3 Team and Organization Management

::card-group
  :::card
  ---
  icon: i-lucide-users
  title: Multi-User Setup
  ---
  Invite team members, set role-based permissions, and implement centralized billing
  :::

  :::card
  ---
  icon: i-lucide-building
  title: Enterprise Features
  ---
  Custom model deployments, dedicated infrastructure, and priority support
  :::
::

## Step 8: Advanced Use Cases and Integration Scenarios

### 8.1 Real-Time Applications

::card-group
  :::card
  ---
  icon: i-lucide-radio
  title: Streaming Responses
  ---
  Implement server-sent events, WebSocket connections, and progressive updates
  :::

  :::card
  ---
  icon: i-lucide-zap
  title: Interactive Applications
  ---
  Build chatbots, real-time code completion, and interactive content generation
  :::
::

### 8.2 High-Volume Production Systems

::card-group
  :::card
  ---
  icon: i-lucide-layers
  title: Batch Processing
  ---
  Process large datasets efficiently with parallel processing and async patterns
  :::

  :::card
  ---
  icon: i-lucide-plug
  title: System Integration
  ---
  Connect with databases, implement middleware, and use message queues
  :::
::

## Summary

By following this guide, your development team can successfully integrate Fireworks AI with CodinIT to leverage ultra-fast AI inference:

::card-group
  :::card
  ---
  icon: i-lucide-user-check
  title: Account Setup
  ---
  Create your account, generate secure API keys, and implement proper access controls
  :::

  :::card
  ---
  icon: i-lucide-gauge
  title: Performance Selection
  ---
  Choose from optimized models based on your speed and capability requirements
  :::

  :::card
  ---
  icon: i-lucide-settings
  title: Configuration
  ---
  Set up the extension with optimal settings for maximum performance
  :::

  :::card
  ---
  icon: i-lucide-trending-up
  title: Optimization
  ---
  Monitor usage, optimize for speed, and manage costs through comprehensive analytics
  :::
::

## Additional Resources

::card-group
  :::card
  ---
  icon: i-lucide-book-open
  title: Documentation
  to: https://docs.fireworks.ai/
  target: _blank
  ---
  Fireworks AI comprehensive documentation
  :::

  :::card
  ---
  icon: i-lucide-code
  title: API Reference
  to: https://docs.fireworks.ai/api-reference
  target: _blank
  ---
  Detailed endpoint documentation
  :::

  :::card
  ---
  icon: i-lucide-play
  title: Model Playground
  to: https://fireworks.ai/models
  target: _blank
  ---
  Interactive model testing environment
  :::

  :::card
  ---
  icon: i-lucide-message-square
  title: Community Discord
  to: https://discord.gg/fireworks-ai
  target: _blank
  ---
  Developer discussions and support
  :::
::

::tip
Start building with Fireworks AI and experience the fastest AI inference available! This guide reflects current capabilities and pricing - visit the official documentation for the most up-to-date information.
::