---
title: Ollama
description: Learn how to use Ollama to run AI models locally on your computer with CodinIT. Get privacy, no API costs, and offline access to powerful AI.
navigation:
  icon: i-lucide-server
---

## What is Ollama?

::callout{color="primary"}
**Ollama** is a free tool that lets you run AI models directly on your own computer instead of using cloud services. This means your conversations stay completely private, you don't pay API fees, and you can use AI even without internet!

Perfect for privacy-conscious developers and those who want full control!
::

## Why Use Ollama with CodinIT?

::card-group
  :::card
  ---
  icon: i-lucide-shield
  title: Complete Privacy
  ---
  Your code and conversations never leave your computer
  :::

  :::card
  ---
  icon: i-lucide-dollar-sign-off
  title: No API Costs
  ---
  Use AI as much as you want without paying per token
  :::

  :::card
  ---
  icon: i-lucide-wifi-off
  title: Works Offline
  ---
  Use AI even when you don't have internet connection
  :::

  :::card
  ---
  icon: i-lucide-settings
  title: Full Control
  ---
  Choose exactly which models to run and how they behave
  :::
::

## What You'll Need

Before setting up Ollama, make sure your computer can handle it:

::card-group
  :::card
  ---
  icon: i-lucide-cpu
  title: Good Processor
  ---
  Modern CPU (Intel i5/AMD Ryzen 5 or better recommended)
  :::

  :::card
  ---
  icon: i-lucide-hard-drive
  title: Enough RAM
  ---
  At least 8GB RAM (16GB+ recommended for larger models)
  :::

  :::card
  ---
  icon: i-lucide-monitor
  title: Graphics Card (Optional)
  ---
  NVIDIA GPU with 4GB+ VRAM for faster performance
  :::

  :::card
  ---
  icon: i-lucide-database
  title: Storage Space
  ---
  5-20GB free space (depending on which models you choose)
  :::
::

::note
**Don't worry if your computer isn't super powerful!** Ollama has smaller models that work well on most modern computers.
::

## Installing Ollama

::steps{level="3"}

### Download Ollama

Go to [ollama.com](https://ollama.com) and download the installer for your operating system (Windows, Mac, or Linux).

### Install and Run

- Run the installer like any other program
- Ollama will start automatically in the background
- You'll see an Ollama icon in your system tray (bottom-right on Windows, top-right on Mac)

### Test Your Installation

Open a terminal/command prompt and type `ollama --version` to make sure it's working.

::

## Choosing Your AI Models

Ollama offers many different models with different capabilities and system requirements:

### Beginner-Friendly Models (4-8GB RAM)

| Model | Size | Intelligence | Best For |
|-------|------|--------------|----------|
| **Llama 3.2 3B** | Small | ‚≠ê‚≠ê‚≠ê | Quick responses, simple tasks |
| **Phi 3 Mini** | Tiny | ‚≠ê‚≠ê | Very fast, basic assistance |
| **Gemma 2B** | Small | ‚≠ê‚≠ê‚≠ê | Balanced performance |

### Powerful Models (16GB+ RAM)

| Model | Size | Intelligence | Best For |
|-------|------|--------------|----------|
| **Llama 3.1 8B** | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê | Great all-around performance |
| **CodeLlama 7B** | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê | Coding and development |
| **Mistral 7B** | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê | Following instructions precisely |

### Expert Models (32GB+ RAM)

| Model | Size | Intelligence | Best For |
|-------|------|--------------|----------|
| **Llama 3.1 70B** | Large | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Complex reasoning, best quality |
| **CodeLlama 34B** | Large | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Advanced coding assistance |

::tip
**Start small!** Begin with **Llama 3.2 3B** or **Phi 3 Mini** to test things out, then upgrade to larger models if you need more capability.
::

## Installing Your First Model

::steps{level="3"}

### Open Terminal/Command Prompt

- Windows: Press Win+R, type `cmd`, press Enter
- Mac: Press Cmd+Space, type `terminal`, press Enter
- Linux: Press Ctrl+Alt+T

### Download a Model

Type this command to download a beginner-friendly model:
```bash
ollama pull llama3.2:3b
```

### Wait for Download

The model will download (this might take 5-20 minutes depending on your internet speed).

### Test Your Model

Once downloaded, test it with:
```bash
ollama run llama3.2:3b
```

::

## Setting Up Ollama in CodinIT

### Step 1: Make Sure Ollama is Running

Check that you see the Ollama icon in your system tray and that you've downloaded at least one model.

### Step 2: Configure CodinIT

::steps{level="4"}

#### Open Project Settings

In your CodinIT project, click the **Settings** button (‚öôÔ∏è).

#### Select Ollama

Choose **"Ollama"** from the AI Provider dropdown.

#### Set the Connection

- **Server URL**: Usually `http://localhost:11434` (this should be the default)
- **Model**: Choose the model you downloaded (e.g., `llama3.2:3b`)

#### Save Settings

Click **Save** to apply your changes.

::

### Step 3: Test Your Connection

::steps{level="4"}

#### Try a Simple Prompt

In your CodinIT chat, ask: *"Help me create a simple webpage"*

#### Check the Response

If everything works, you'll get a response from your local AI model!

#### Troubleshoot if Needed

If it doesn't work, make sure Ollama is running and you have the model downloaded.

::

## Managing Your Models

### Downloading New Models

::tabs
  :::tabs-item{label="Popular Models" icon="i-lucide-star"}
  ```bash
  # For general use
  ollama pull llama3.1:8b
  
  # For coding
  ollama pull codellama:7b
  
  # For fast responses  
  ollama pull phi3:mini
  ```
  :::

  :::tabs-item{label="Model Commands" icon="i-lucide-terminal"}
  ```bash
  # See available models
  ollama list
  
  # Remove a model
  ollama rm model-name
  
  # Update a model
  ollama pull model-name
  ```
  :::
::

### Storage Management

::field-group
  :::field{name="Check Model Sizes" type="tip"}
  Use `ollama list` to see how much space each model uses
  :::

  :::field{name="Remove Unused Models" type="tip"}
  Delete models you don't use with `ollama rm model-name` to free up space
  :::

  :::field{name="Model Storage Location" type="tip"}
  Models are stored in your user directory - check Ollama docs for exact location
  :::
::

## Understanding Performance

### What Affects Speed

::card-group
  :::card
  ---
  icon: i-lucide-zap
  title: Model Size
  ---
  Smaller models respond faster but may be less capable
  :::

  :::card
  ---
  icon: i-lucide-cpu
  title: Your Hardware
  ---
  Better CPU/GPU = faster responses
  :::

  :::card
  ---
  icon: i-lucide-memory-stick
  title: Available RAM
  ---
  More RAM lets you run larger, smarter models
  :::
::

### Performance Tips

::tabs
  :::tabs-item{label="Speed Optimization" icon="i-lucide-gauge"}
  - Use smaller models for quick tasks
  - Close other heavy programs while using AI
  - Consider GPU acceleration if you have NVIDIA graphics
  - Keep conversations shorter for faster responses
  :::

  :::tabs-item{label="Quality Optimization" icon="i-lucide-star"}
  - Use larger models for complex tasks
  - Make sure you have enough RAM available
  - Be specific in your prompts
  - Allow time for the model to "think"
  :::
::

## Comparing Local vs Cloud AI

### When to Use Ollama (Local)

::card-group
  :::card
  ---
  icon: i-lucide-shield
  title: Privacy Matters
  ---
  Working with sensitive code or personal projects
  :::

  :::card
  ---
  icon: i-lucide-dollar-sign-off
  title: Cost Control
  ---
  Heavy usage that would be expensive with API fees
  :::

  :::card
  ---
  icon: i-lucide-wifi-off
  title: Offline Work
  ---
  Need to work without internet connection
  :::

  :::card
  ---
  icon: i-lucide-settings
  title: Full Control
  ---
  Want to experiment with different models and settings
  :::
::

### When to Use Cloud AI

::card-group
  :::card
  ---
  icon: i-lucide-zap
  title: Need Speed
  ---
  Want the fastest possible responses
  :::

  :::card
  ---
  icon: i-lucide-brain
  title: Maximum Intelligence
  ---
  Need the most capable AI models available
  :::

  :::card
  ---
  icon: i-lucide-smartphone
  title: Limited Hardware
  ---
  Computer doesn't have enough power for local models
  :::

  :::card
  ---
  icon: i-lucide-plug
  title: Easy Setup
  ---
  Want simple setup without downloading large files
  :::
::

## Getting Help

Need more assistance?

::card-group
  :::card
  ---
  icon: i-lucide-external-link
  title: Ollama Documentation
  to: https://ollama.com/
  target: _blank
  ---
  Official Ollama guides and model library
  :::

  :::card
  ---
  icon: i-lucide-github
  title: Ollama GitHub
  to: https://github.com/ollama/ollama
  target: _blank
  ---
  Technical documentation and community support
  :::

  :::card
  ---
  icon: i-lucide-help-circle
  title: CodinIT Support
  ---
  Contact our support team for CodinIT-specific help
  :::
::

::tip
üè† **Ready to go local?** With Ollama connected to your CodinIT project, you now have private, cost-free AI that runs entirely on your computer. Perfect for when privacy and control matter most!
::