---
title: "Groq"
description: "Configure Groq's ultra-fast LPU inference for models from OpenAI, Meta, and DeepSeek."
---

Groq provides ultra-fast AI inference through custom LPU™ (Language Processing Unit) architecture. Hosts open-source models from OpenAI, Meta, DeepSeek, and others.

**Website:** [https://groq.com/](https://groq.com/)

## Getting an API Key

1. Go to [Groq Console](https://console.groq.com/) and sign in
2. Navigate to API Keys section
3. Create a new API key and name it (e.g., "CodinIT")
4. Copy the key immediately - you won't see it again

## Configuration

1. Click the settings icon (⚙️) in CodinIT
2. Select "Groq" as the API Provider
3. Paste your API key
4. Choose your model

## Supported Models

- `llama-3.3-70b-versatile` (Meta) - 131K context
- `openai/gpt-oss-120b` (OpenAI) - 131K context
- `moonshotai/kimi-k2-instruct` - 1T parameters with caching
- `deepseek-r1-distill-llama-70b` - Reasoning optimized
- `qwen/qwen3-32b` (Alibaba) - Q&A enhanced
- `meta-llama/llama-4-maverick-17b-128e-instruct`

## Key Features

- **Ultra-fast inference:** Sub-millisecond latency with LPU architecture
- **Large context:** Up to 131K tokens
- **Prompt caching:** Available on select models
- **Vision support:** Available on select models

Learn more about [LPU architecture](https://groq.com/blog/inside-the-lpu-deconstructing-groq-speed).

## Notes

- **Speed:** Optimized for single-request latency
- **Pricing:** See [Groq Pricing](https://groq.com/pricing)