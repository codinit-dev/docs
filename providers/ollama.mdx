---
title: "Ollama"
description: "Run AI models locally with Ollama for privacy and offline access."
---

Run models locally using Ollama for privacy, offline access, and control. Requires initial setup and sufficient hardware.

**Website:** [https://ollama.com/](https://ollama.com/)

## Setup

1. **Install Ollama:** Download from [ollama.com](https://ollama.com/) and install
2. **Start Ollama:** Run `ollama serve` in terminal
3. **Download a model:**
   ```bash
   ollama pull qwen2.5-coder:32b
   ```
4. **Configure context window:**
   ```bash
   ollama run qwen2.5-coder:32b
   /set parameter num_ctx 32768
   /save your_custom_model_name
   ```

## Configuration in CodinIT

1. Click the settings icon (⚙️) in CodinIT
2. Select "ollama" as the API Provider
3. Enter your saved model name
4. (Optional) Set base URL if not using default `http://localhost:11434`

## Recommended Models

- `qwen2.5-coder:32b` - Excellent for coding
- `codellama:34b-code` - High quality, large size
- `deepseek-coder:6.7b-base` - Effective for coding
- `llama3:8b-instruct-q5_1` - General tasks

See [Ollama model library](https://ollama.com/library) for full list.

## Notes

- **Context window:** Minimum 12,000 tokens recommended, 32,000 ideal
- **Resource demands:** Large models require significant system resources
- **Offline capability:** Works without internet after model download
- **Performance:** May be slow on average hardware
