---
title: "Context Window Guide"
description: "How much the AI can remember at once"
---

## What is a Context Window?

A context window is how much text the AI can look at and remember at one time. Think of it like the AI's short-term memory.

<Note>
  **What are tokens?** Tokens are small pieces of text. About 4 letters = 1 token. So "hamburger" is 2 tokens: "ham" and "burger". When we say "128K tokens," that means the AI can remember about 96,000 words at once.
</Note>

<Tip>
  **Important**: Bigger context windows let the AI see more of your project, but they cost more money and take longer.
</Tip>

### Size Guide

| Size            | Tokens | About How Many Words | Good For                  |
| --------------- | ------ | ----------------- | ------------------------- |
| **Small**       | 8K-32K | 6,000-24,000      | One file, quick fixes |
| **Medium**      | 128K   | ~96,000           | Most projects      |
| **Large**       | 200K   | ~150,000          | Big projects         |
| **Extra Large** | 400K+  | ~300,000+         | Whole apps       |
| **Huge**     | 1M+    | ~750,000+         | Multiple projects    |

### Different AI Models

| AI Model                 | Context Window | Actually Works Well\* | Notes                          |
| --------------------- | -------------- | ------------------ | ------------------------------ |
| **Claude Sonnet 4.5** | 1M tokens      | ~500K tokens       | Best for big projects   |
| **GPT-5**             | 400K tokens    | ~300K tokens       | Has three different modes |
| **Gemini 2.5 Pro**    | 1M+ tokens     | ~600K tokens       | Great for reading documents        |
| **DeepSeek V3**       | 128K tokens    | ~100K tokens       | Good for most things         |
| **Qwen3 Coder**       | 256K tokens    | ~200K tokens       | Nice balance                   |

\*The AI works best up to this point. After that, it might start forgetting earlier parts of your chat.

### What Uses Up the Context Window

1. **Your messages** - Everything you and the AI say in the chat
2. **Your files** - Files the AI reads from your project
3. **Command results** - Output from commands that run
4. **System instructions** - CodinIT's background instructions (uses very little)

### How to Save Context Space

#### 1. Start fresh for new features

```mdx
/new - Start a new chat with empty context
```

Why this helps:

- You get the full context window
- No old, irrelevant messages
- AI focuses better

#### 2. Only include files you need

Instead of including everything:

- `@filename.ts` - Only mention files when necessary
- Use search instead of reading big files
- Reference specific parts, not whole files

#### 3. Turn on auto-compact

CodinIT can automatically shorten long conversations:

- Go to Settings → Features → Auto-compact
- Keeps important stuff
- Uses fewer tokens

## Warning Signs

### How to Tell You're Running Out of Space

| Warning Sign                  | What It Means                 | What to Do                              |
| ----------------------------- | ----------------------------- | ------------------------------------- |
| **"Context window exceeded"** | You hit the limit            | Start a new chat or turn on auto-compact |
| **Slow responses**          | AI is struggling | Include fewer files                 |
| **AI repeats itself**    | Context is too fragmented         | Start fresh             |
| **AI forgets recent changes**    | Too much context              | Use checkpoints      |

### Tips by Project Size

#### Small projects (less than 50 files)

- Any AI model works fine
- Include files freely
- Don't worry about optimization

#### Medium projects (50-500 files)

- Use AI with 128K+ context
- Only include files you're working on
- Start new chats between features

#### Large projects (500+ files)

- Use AI with 200K+ context
- Focus on one part at a time
- Search instead of reading many files
- Break work into smaller pieces

## Advanced context management

### Plan/Act mode optimization

Leverage Plan/Act mode for better context usage:

- **Plan Mode**: Use smaller context for discussion and planning
- **Act Mode**: Include necessary files when you're ready to write code

Configuration example:

```
Plan Mode: DeepSeek V3 (128K) - Lower cost planning
Act Mode: Claude Sonnet (1M) - Maximum context for coding
```

### Context pruning strategies

These are ways CodinIT can reduce the amount of text in your context window:

1. **Temporal pruning**: Remove older parts of your conversation that are no longer relevant
2. **Semantic pruning**: Keep only the code sections related to your current task
3. **Hierarchical pruning**: Keep the big picture but remove fine details

### Token counting tips

#### Rough estimates

- **1 token ≈ 0.75 words** (so 1,000 tokens is about 750 words)
- **1 token ≈ 4 characters**
- **100 lines of code ≈ 500-1000 tokens**

#### File size guidelines

| File Type      | Tokens per KB |
| -------------- | ------------- |
| **Code**       | ~250-400      |
| **JSON**       | ~300-500      |
| **Markdown**   | ~200-300      |
| **Plain text** | ~200-250      |

## Context window FAQ

### Q: Why do responses get worse with very long conversations?

**A:** Models can lose focus with too much context. The "effective window" is typically 50-70% of the advertised limit.

### Q: Should I use the largest context window available?

**A:** Not always. Larger contexts increase cost and can reduce response quality. Match the context to your task size.

### Q: How can I tell how much context I'm using?

**A:** CodinIT shows token usage in the interface. Watch for the context meter approaching limits.

### Q: What happens when I exceed the context limit?

**A:** CodinIT will either:

- Automatically compact the conversation (if enabled)
- Show an error and suggest starting a new task
- Truncate older messages (with warning)

## Recommendations by use case

| Use Case                | Recommended Context | Model Suggestion  |
| ----------------------- | ------------------- | ----------------- |
| **Quick fixes**         | 32K-128K            | DeepSeek V3       |
| **Feature development** | 128K-200K           | Qwen3 Coder       |
| **Large refactoring**   | 400K+               | Claude Sonnet 4.5 |
| **Code review**         | 200K-400K           | GPT-5             |
| **Documentation**       | 128K                | Any budget model  |